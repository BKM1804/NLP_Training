{"cells":[{"cell_type":"code","execution_count":41,"metadata":{"executionInfo":{"elapsed":15651,"status":"ok","timestamp":1712487049577,"user":{"displayName":"Bùi Khắc Minh","userId":"03345941564288769937"},"user_tz":-420},"id":"OOFvhq3_dTMz"},"outputs":[],"source":["!wget http://nlp.stanford.edu/data/glove.6B.zip\n","!unzip glove*.zip\n","!rm glove.6B.zip\n","!tar -xzf /content/drive/MyDrive/NLP_Training/yelp_review_full_csv.tar.gz"]},{"cell_type":"code","execution_count":99,"metadata":{"executionInfo":{"elapsed":3391,"status":"ok","timestamp":1712489540924,"user":{"displayName":"Bùi Khắc Minh","userId":"03345941564288769937"},"user_tz":-420},"id":"Smw5FrmCp4aB"},"outputs":[],"source":["import torch\n","from torch import nn\n","from torch.nn import GRU\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader , Dataset\n","from torch.optim import Adam\n","from torch.nn import CrossEntropyLoss\n","from transformers import get_scheduler\n","import os\n","import numpy as np\n","import re\n","from tqdm import tqdm\n","import pandas as pd"]},{"cell_type":"code","execution_count":179,"metadata":{"executionInfo":{"elapsed":358,"status":"ok","timestamp":1712496660542,"user":{"displayName":"Bùi Khắc Minh","userId":"03345941564288769937"},"user_tz":-420},"id":"I4cyW88MhUhg"},"outputs":[],"source":["\n","\n","class WordAttn(nn.Module):\n","  def __init__(self , embed_size :int  , hidden_size : int ):\n","    super(WordAttn, self).__init__()\n","    self.gru = GRU(input_size = embed_size , hidden_size = hidden_size , bidirectional=True)\n","    self.MLP = nn.Linear(2*hidden_size , 2*hidden_size , bias = True)\n","    self.word_context = nn.Parameter(torch.Tensor(2 * hidden_size, 1))\n","  def forward(self , x):\n","    x , _ = self.gru(x.float())\n","    u_it = self.MLP(x)\n","    u_it = torch.matmul(u_it , self.word_context)\n","    u_it = F.tanh(u_it)\n","    out = F.softmax(u_it , dim = 0)\n","    out = torch.mul(out , x)\n","    out = torch.sum(out, dim = 0)\n","    return out\n","\n","\n","class SentenceAttn(nn.Module):\n","  def __init__(self , embed_size :int  , hidden_size : int):\n","    super(SentenceAttn , self).__init__()\n","    self.gru = GRU(input_size = embed_size , hidden_size = hidden_size , bidirectional=True)\n","    self.MLP = nn.Linear(2*hidden_size , 2*hidden_size , bias = True)\n","    self.word_context = nn.Parameter(torch.Tensor(2 * hidden_size, 1))\n","  def forward(self , x):\n","    x , _ = self.gru(x.float())\n","    u_it = self.MLP(x)\n","    u_it = torch.matmul(u_it , self.word_context)\n","    out = F.softmax(u_it , dim = 0)\n","    out = torch.mul(out , x)\n","\n","\n","    out = torch.sum(out, dim = 0)\n","\n","    return out\n","\n","class MyModel(nn.Module):\n","  def __init__(self , embed_size : int , hidden_size : int):\n","    super(MyModel , self).__init__()\n","    self.WordAttn = WordAttn(embed_size , hidden_size)\n","    self.SentenceAttn = SentenceAttn(hidden_size *2 , hidden_size)\n","    self.MLP = nn.Linear(hidden_size*2 , 5)\n","  def forward(self , input):\n","    out_sent = []\n","    input = input.permute(1, 0, 2 , 3)\n","\n","    for i in input :\n","      out = self.WordAttn(i.permute(1, 0 , 2))\n","      out_sent.append(out)\n","    out_sent = torch.stack(out_sent )\n","\n","\n","    out_sent = self.SentenceAttn(out_sent)\n","    out_sent = self.MLP(out_sent)\n","    out_sent = F.softmax(out_sent)\n","    return out_sent"]},{"cell_type":"code","execution_count":96,"metadata":{"executionInfo":{"elapsed":383,"status":"ok","timestamp":1712489227960,"user":{"displayName":"Bùi Khắc Minh","userId":"03345941564288769937"},"user_tz":-420},"id":"croKJWI2pwnx"},"outputs":[],"source":["\n","class Mydataset(Dataset):\n","  def __init__(self , data  , label):\n","    self.data = data\n","    self.label = label\n","  def __len__(self):\n","    return len(self.data)\n","  def __getitem__(self, idx):\n","    dt = {\"data\": self.data[idx] , 'y' : self.label[idx]}\n","    return dt\n"]},{"cell_type":"code","source":["\n","embed_size = 50\n","max_len_sen = 5\n","max_word = 5\n","def load_glove(glove_file):\n","    with open(glove_file, 'r') as f:\n","        words = set()\n","        word_to_vec_map = {}\n","        for line in f:\n","            line = line.strip().split()\n","            curr_word = line[0]\n","            words.add(curr_word)\n","            word_to_vec_map[curr_word] = np.array([float(x) for x in line[1:]])\n","    return words, word_to_vec_map\n","def split_text_to_sentences(text):\n","    # Sử dụng biểu thức chính quy để tách văn bản thành các câu\n","    sentences = re.split(r'[.!?]+', text)\n","\n","    # Loại bỏ các câu trống\n","    sentences = [sentence.strip() for sentence in sentences if sentence.strip()]\n","\n","    return sentences\n","def split_sentence_to_words(sentence):\n","    # Thay thế các dấu phẩy bằng dấu cách\n","    sentence = sentence.replace(',', ' , ')\n","\n","    # Tách câu thành các từ bằng cách sử dụng phương thức split()\n","    words = sentence.split()\n","\n","    return words\n","def get_data(word_to_vec_map , root_path : str  ):\n","  train_csv_path = os.path.join(root_path , \"train.csv\")\n","  test_csv_path = os.path.join(root_path , 'test.csv')\n","  dt_train = pd.read_csv(train_csv_path ,nrows=50000).values\n","  dt_test = pd.read_csv(test_csv_path).values\n","  embed_train = []\n","  label_train = []\n","  for x in tqdm(dt_train,desc = \"train_process\"):\n","    label = x[0]\n","    sentences = x[1].strip()\n","    sentences = split_text_to_sentences(sentences)\n","    encode = []\n","    for st in sentences:\n","        sen2vec = []\n","        sen2vec = [word_to_vec_map[word.lower()] if word in word_to_vec_map else np.zeros((embed_size))  for word in split_sentence_to_words(st)]\n","        if(len(sen2vec) > max_word) :\n","          sen2vec = sen2vec[0:max_word]\n","        else :\n","          pad = [np.zeros((embed_size)) for _ in range(max_word - len(sen2vec) )]\n","          sen2vec.extend(pad)\n","        sen2vec = np.stack(sen2vec)\n","        encode.append(sen2vec)\n","    if(len(encode) > max_len_sen) :\n","      encode = encode[0:max_len_sen]\n","    else :\n","      pad = [np.zeros((max_word ,embed_size )) for _ in range(max_len_sen - len(encode))]\n","      encode.extend(pad)\n","    try:\n","      encode = np.stack(encode)\n","    except:\n","      continue\n","    embed_train.append(encode)\n","    label_train.append(label)\n","\n","\n","\n","  embed_test = []\n","  label_test = []\n","  for x in tqdm(dt_test , desc = \" test_process\"):\n","    label = x[0]\n","    sentences = x[1].strip()\n","    sentences = split_text_to_sentences(sentences)\n","    encode = []\n","    for st in sentences:\n","        sen2vec = []\n","        sen2vec = [word_to_vec_map[word.lower()] if word in word_to_vec_map else np.zeros((embed_size))  for word in split_sentence_to_words(st)]\n","        if(len(sen2vec) > max_word) :\n","          sen2vec = sen2vec[0:max_word]\n","        else :\n","          pad = [np.zeros((embed_size)) for _ in range(max_word - len(sen2vec))]\n","          sen2vec.extend(pad)\n","        sen2vec = np.stack(sen2vec)\n","        encode.append(sen2vec)\n","    if(len(encode) > max_len_sen) :\n","      encode = encode[0:max_len_sen]\n","    else :\n","      pad = [np.zeros((max_word ,embed_size )) for _ in range(max_len_sen - len(encode))]\n","      encode.extend(pad)\n","    try:\n","      encode = np.stack(encode)\n","    except:\n","      continue\n","    embed_test.append(encode)\n","    label_test.append(label)\n","  return embed_train ,label_train ,  embed_test , label_test\n","\n"],"metadata":{"id":"aSYCeeujsXSi","executionInfo":{"status":"ok","timestamp":1712489789220,"user_tz":-420,"elapsed":345,"user":{"displayName":"Bùi Khắc Minh","userId":"03345941564288769937"}}},"execution_count":103,"outputs":[]},{"cell_type":"code","source":["words, word_to_vec_map = load_glove(\"/content/glove.6B.50d.txt\")"],"metadata":{"id":"kjcukzZK5WCh","executionInfo":{"status":"ok","timestamp":1712485775081,"user_tz":-420,"elapsed":11646,"user":{"displayName":"Bùi Khắc Minh","userId":"03345941564288769937"}}},"execution_count":36,"outputs":[]},{"cell_type":"code","source":["embed_train ,label_train ,  embed_test , label_test = get_data( word_to_vec_map , '/content/yelp_review_full_csv' )"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WBgXT5Zp48Au","executionInfo":{"status":"ok","timestamp":1712489831513,"user_tz":-420,"elapsed":39500,"user":{"displayName":"Bùi Khắc Minh","userId":"03345941564288769937"}},"outputId":"46e8ccb7-989d-4566-ffd3-e115c1dbfa89"},"execution_count":104,"outputs":[{"output_type":"stream","name":"stderr","text":["train_process: 100%|██████████| 50000/50000 [00:16<00:00, 3029.50it/s]\n"," test_process: 100%|██████████| 49999/49999 [00:21<00:00, 2321.56it/s]\n"]}]},{"cell_type":"code","source":["train_set = Mydataset(embed_train , label_train)\n","Train_loader = DataLoader(train_set , batch_size = 32 , shuffle = True)"],"metadata":{"id":"1A4YumVpzwBf","executionInfo":{"status":"ok","timestamp":1712489849171,"user_tz":-420,"elapsed":344,"user":{"displayName":"Bùi Khắc Minh","userId":"03345941564288769937"}}},"execution_count":106,"outputs":[]},{"cell_type":"code","source":["num_epochs = 5\n","model = MyModel(50 , 128)\n","loss_fc = CrossEntropyLoss(reduce=True)\n","opt = Adam(model.parameters() , lr = 0.001 , weight_decay = 0)\n","for epoch in range(num_epochs):\n","  pbar = tqdm(Train_loader , desc=\"training\")\n","\n","  total_loss = 0\n","  model.train()\n","  for i , data in enumerate(pbar):\n","      opt.zero_grad()\n","      out = model(data['data'])\n","      out.cpu()\n","      loss = loss_fc(out.squeeze() , torch.nn.functional.one_hot(data['y']-1, 5).to(torch.float))\n","      total_loss += loss.item()\n","      loss.backward()\n","      opt.step()\n","      pbar.set_description(\"Epoch: {}, Loss: {:4f}\".format(epoch + 1, total_loss/(i+1)))\n"],"metadata":{"id":"dzvTp1OHtEVn"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"mount_file_id":"1okx8PW1NOBCW08mBMimuNhTSG245kySG","authorship_tag":"ABX9TyMLSiu8aL8XuvC9HYCXEUX4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}