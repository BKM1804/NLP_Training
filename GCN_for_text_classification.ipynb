{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "soeB_A1N7r22",
        "outputId": "0d992755-9c93-465e-dea9-1e579eefff94"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-04-21 14:31:51--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2024-04-21 14:31:51--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2024-04-21 14:31:51--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  5.02MB/s    in 2m 38s  \n",
            "\n",
            "2024-04-21 14:34:30 (5.19 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n",
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ]
        }
      ],
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove*.zip\n",
        "!rm glove.6B.zip\n",
        "!tar -xzf /content/drive/MyDrive/NLP_Training/yelp_review_full_csv.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m_NiX0HnCLZA",
        "outputId": "de34e251-948c-4983-f81a-1bae642aa920"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/data\n",
            "/content\n"
          ]
        }
      ],
      "source": [
        "!mkdir ./data\n",
        "%cd data\n",
        "!mkdir corpus\n",
        "%cd .."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a9QmXSszkApg",
        "outputId": "dbcebf86-b77d-4e2c-c2c2-c4df0c2e1ecd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'the': array([0.38408524, 0.28108867, 0.26710379, 0.38408524]), 'first': array([0.58028582, 0.        , 0.        , 0.58028582]), 'document': array([0.46979139, 0.6876236 , 0.        , 0.46979139]), 'is': array([0.38408524, 0.28108867, 0.26710379, 0.38408524]), 'second': array([0.        , 0.53864762, 0.        , 0.        ]), 'third': array([0.        , 0.        , 0.51184851, 0.        ]), 'and': array([0.        , 0.        , 0.51184851, 0.        ]), 'one': array([0.        , 0.        , 0.51184851, 0.        ]), 'this': array([0.38408524, 0.28108867, 0.26710379, 0.38408524])}\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Dữ liệu văn bản\n",
        "corpus = [\n",
        "    'This is the first document.',\n",
        "    'This document is the second document.',\n",
        "    'And this is the third one.',\n",
        "    'Is this the first document?'\n",
        "]\n",
        "vocabulary = ['the', 'first', 'document', 'is', 'second', 'third', 'and', 'one', 'this']\n",
        "# Khởi tạo TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer(vocabulary=vocabulary)\n",
        "\n",
        "# Fit và transform dữ liệu\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "\n",
        "# In ma trận TF-IDF\n",
        "vocabulary = vectorizer.vocabulary_\n",
        "\n",
        "# In danh sách các từ (features)\n",
        "word_embeddings = {}\n",
        "for word, index in vocabulary.items():\n",
        "    word_embeddings[word] = X[:, index].toarray().squeeze()\n",
        "\n",
        "print(word_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "iTadU1Tc_3P4"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "# from utils import load_corpus\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "CBkwLdMt_Rzk"
      },
      "outputs": [],
      "source": [
        "def read_data(root_path):\n",
        "    train_csv_path = os.path.join(root_path , \"train.csv\")\n",
        "    test_csv_path = os.path.join(root_path , 'test.csv')\n",
        "    dt_train = pd.read_csv(train_csv_path ,nrows=500).values\n",
        "    dt_test = pd.read_csv(test_csv_path).values\n",
        "    text_train = []\n",
        "    label_train = []\n",
        "    for x in dt_train:\n",
        "        text_train.append(x[1].strip())\n",
        "        label_train.append(str(x[0]))\n",
        "    text_test = []\n",
        "    label_test = []\n",
        "    for x in dt_test:\n",
        "        text_test.append(x[1].strip())\n",
        "        label_test.append(str(x[0]))\n",
        "    return text_train , label_train , text_test , label_test\n",
        "\n",
        "def prepare_data():\n",
        "    dataset_name = 'My_dataset'\n",
        "    sentences , labels , st_test , st_test = read_data('/content/yelp_review_full_csv')\n",
        "    train_or_test_list = ['train', 'test']\n",
        "    meta_data_list = []\n",
        "    for i in range(len(sentences)):\n",
        "        # meta = str(i) + '\\t' + train_or_test_list[i] + '\\t' + labels[i]\n",
        "        meta = str(i) + '\\t' + \"train\" + '\\t' + labels[i]\n",
        "        meta_data_list.append(meta)\n",
        "    meta_data_str = '\\n'.join(meta_data_list)\n",
        "\n",
        "    f = open('data/' + dataset_name + '.txt', 'w')\n",
        "    f.write(meta_data_str)\n",
        "    f.close()\n",
        "\n",
        "    corpus_str = '\\n'.join(sentences)\n",
        "\n",
        "    f = open('data/corpus/' + dataset_name + '.txt', 'w')\n",
        "    f.write(corpus_str)\n",
        "    f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "OF9GQotfBEmR"
      },
      "outputs": [],
      "source": [
        "prepare_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "mXpafmbyJ2yd"
      },
      "outputs": [],
      "source": [
        "def clean_str(string):\n",
        "    \"\"\"\n",
        "    Tokenization/string cleaning for all datasets except for SST.\n",
        "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
        "    \"\"\"\n",
        "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
        "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
        "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
        "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
        "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
        "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
        "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
        "    string = re.sub(r\",\", \" , \", string)\n",
        "    string = re.sub(r\"!\", \" ! \", string)\n",
        "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
        "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
        "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
        "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
        "    return string.strip().lower()\n",
        "def remove():\n",
        "    dataset  = 'My_dataset'\n",
        "    nltk.download('stopwords')\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    print(stop_words)\n",
        "    doc_content_list = []\n",
        "    f = open('data/corpus/' + dataset + '.txt', 'rb')\n",
        "    # f = open('data/wiki_long_abstracts_en_text.txt', 'r')\n",
        "    for line in f.readlines():\n",
        "        doc_content_list.append(line.strip().decode('latin1'))\n",
        "    f.close()\n",
        "\n",
        "\n",
        "    word_freq = {}  # to remove rare words\n",
        "\n",
        "    for doc_content in doc_content_list:\n",
        "        temp = clean_str(doc_content)\n",
        "        words = temp.split()\n",
        "        for word in words:\n",
        "            if word in word_freq:\n",
        "                word_freq[word] += 1\n",
        "            else:\n",
        "                word_freq[word] = 1\n",
        "\n",
        "    clean_docs = []\n",
        "    for doc_content in doc_content_list:\n",
        "        temp = clean_str(doc_content)\n",
        "        words = temp.split()\n",
        "        doc_words = []\n",
        "        for word in words:\n",
        "            if dataset == 'mr':\n",
        "                doc_words.append(word)\n",
        "            elif word not in stop_words and word_freq[word] >= 5:\n",
        "                doc_words.append(word)\n",
        "\n",
        "        doc_str = ' '.join(doc_words).strip()\n",
        "        clean_docs.append(doc_str)\n",
        "\n",
        "    clean_corpus_str = '\\n'.join(clean_docs)\n",
        "\n",
        "    f = open('data/corpus/' + dataset + '.clean.txt', 'w')\n",
        "    f.write(clean_corpus_str)\n",
        "    f.close()\n",
        "    min_len = 10000\n",
        "    aver_len = 0\n",
        "    max_len = 0\n",
        "\n",
        "    f = open('data/corpus/' + dataset + '.clean.txt', 'r')\n",
        "    lines = f.readlines()\n",
        "    for line in lines:\n",
        "        line = line.strip()\n",
        "        temp = line.split()\n",
        "        aver_len = aver_len + len(temp)\n",
        "        if len(temp) < min_len:\n",
        "            min_len = len(temp)\n",
        "        if len(temp) > max_len:\n",
        "            max_len = len(temp)\n",
        "    f.close()\n",
        "    aver_len = 1.0 * aver_len / len(lines)\n",
        "    print('min_len : ' + str(min_len))\n",
        "    print('max_len : ' + str(max_len))\n",
        "    print('average_len : ' + str(aver_len))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Un82l5u0QJGk",
        "outputId": "e7d8e118-3c6b-43d1-95f7-3a6bd7cbe6b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'who', 'so', 'myself', \"don't\", 'has', \"haven't\", 'own', 'those', 'few', 'after', 'he', 'can', 'hasn', 'wouldn', 'their', 'down', 'then', \"didn't\", \"you'd\", 'o', 'ourselves', 'shouldn', 'why', 'am', 'your', 'over', 'all', 'than', \"weren't\", 'it', \"mightn't\", 'them', \"won't\", 'where', 'against', 'been', 'other', \"should've\", 'that', 'yourselves', 'haven', 'some', \"needn't\", 'whom', 'to', \"mustn't\", 'further', 'themselves', 'through', 'our', 'aren', 'hadn', \"aren't\", 'again', 'these', 'until', 'hers', 'how', 've', 'didn', 'weren', 'was', 'at', 'him', 'have', 'do', \"you're\", 'mustn', 'an', 'from', 'ma', 'for', 'very', 'if', 'more', 'on', 'because', 'any', 'same', 'couldn', 'her', 'as', \"you'll\", 'in', 'which', \"wouldn't\", \"couldn't\", 'is', 'herself', 'this', 'below', 'but', \"hadn't\", 'having', 'll', 'with', 'ain', 'out', 'm', 'when', 're', 'only', 'his', 'before', \"she's\", 's', 'of', 'needn', 'nor', 'while', 'or', 'we', 'doing', 'doesn', 'me', 'be', \"isn't\", \"it's\", 'd', 'does', 'here', 'don', 'y', 'isn', 'wasn', 'most', \"you've\", 'not', 'into', 'too', 'under', 'you', 'each', 'being', 'are', 'off', 'during', \"shan't\", 'she', 'above', \"that'll\", 'what', \"wasn't\", 'they', 'won', 'its', 'were', 'the', 'and', 'a', 'shan', 'himself', 'ours', 'once', 'there', 'both', 'mightn', 'yours', 'just', 'yourself', 't', 'had', 'about', 'my', 'should', 'such', 'will', 'no', 'now', \"shouldn't\", \"doesn't\", 'itself', 'between', \"hasn't\", 'theirs', 'i', 'by', 'did', 'up'}\n",
            "min_len : 0\n",
            "max_len : 378\n",
            "average_len : 64.9\n"
          ]
        }
      ],
      "source": [
        "remove()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "U5YTbJjNtLKy"
      },
      "outputs": [],
      "source": [
        "def load_glove(glove_file):\n",
        "    with open(glove_file, 'r') as f:\n",
        "        words = set()\n",
        "        word_to_vec_map = {}\n",
        "        for line in f:\n",
        "            line = line.strip().split()\n",
        "            curr_word = line[0]\n",
        "            words.add(curr_word)\n",
        "            word_to_vec_map[curr_word] = np.array([float(x) for x in line[1:]])\n",
        "    return words, word_to_vec_map\n",
        "words, word_to_vec_map = load_glove(\"/content/glove.6B.50d.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {
        "id": "7flGxCPzKiqa"
      },
      "outputs": [],
      "source": [
        "embed_size = 50\n",
        "def get_docs():\n",
        "    docs = []\n",
        "    label = []\n",
        "    with open('/content/data/corpus/My_dataset.clean.txt' , \"r\") as f:\n",
        "        data = f.readlines()\n",
        "        for doc in data:\n",
        "            docs.append(doc.strip())\n",
        "    with open('/content/data/My_dataset.txt' , 'r') as f:\n",
        "        lines = f.readlines()\n",
        "        for l in lines:\n",
        "            label.append(int(l.strip().split()[-1]))\n",
        "    return docs , label\n",
        "def doc_embedding(doc, word_embeddings):\n",
        "    \"\"\"\n",
        "    Tính document embedding bằng cách lấy trung bình các word embedding trong document.\n",
        "\n",
        "    Args:\n",
        "        doc (str): Tài liệu (document).\n",
        "        word_embeddings (dict): Dictionary chứa word embedding tương ứng với mỗi từ.\n",
        "\n",
        "    Returns:\n",
        "        dict: Dictionary chứa document embedding, với key là 0 và value là document embedding.\n",
        "    \"\"\"\n",
        "    # Tách từng từ trong document\n",
        "    words = doc.split()\n",
        "\n",
        "    # Tính trung bình của các word embedding\n",
        "    doc_embed = []\n",
        "    for word in words:\n",
        "        if word in word_embeddings:\n",
        "            doc_embed.append(word_embeddings[word])\n",
        "        else:\n",
        "            # Nếu từ không có trong word_embeddings, sử dụng vector 0 với cùng kích thước\n",
        "            doc_embed.append([0] *embed_size)\n",
        "    try:\n",
        "        doc_embed = np.sum(doc_embed , axis= 0)/len(doc_embed)\n",
        "    except:\n",
        "        import pdb\n",
        "        pdb.set_trace()\n",
        "    return doc_embed\n",
        "def sliding(window_size : int =20):\n",
        "    docs , label= get_docs()\n",
        "    vocabs = set()\n",
        "    sliding_window = []\n",
        "    for doc in docs :\n",
        "        # print(doc)\n",
        "        doc = doc.split()\n",
        "        vocabs.update(doc)\n",
        "        if(len(doc) < window_size) :\n",
        "            sliding_window.append(doc)\n",
        "        else :\n",
        "            for i in range(0 , len(doc) - window_size):\n",
        "                sliding_window.append(doc[i : i + window_size])\n",
        "    vocabs = list(vocabs)\n",
        "    size_vocab = len(vocabs)\n",
        "    node_id = {}\n",
        "    features = []\n",
        "    lb= []\n",
        "    for idx , vocab in enumerate(vocabs):\n",
        "        node_id[vocab] = idx\n",
        "        if vocab not in word_to_vec_map:\n",
        "            word_to_vec_map[vocab] = np.array([0]*embed_size)\n",
        "        features.append(word_to_vec_map[vocab])\n",
        "\n",
        "    for idx , doc in enumerate(docs):\n",
        "        node_id[doc] = idx  + size_vocab + 1\n",
        "        # print(doc_embedding(doc , word_to_vec_map).shape , \"123\")\n",
        "        # break\n",
        "\n",
        "        if doc==\"\":\n",
        "            continue\n",
        "        features.append(doc_embedding(doc , word_to_vec_map))\n",
        "        lb.append(label[idx])\n",
        "    return node_id ,  features ,sliding_window , size_vocab , vocabs , lb\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZZ6jhOixss_a",
        "outputId": "fca3171c-a439-4d1a-ebe5-1b4d825dc7a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1391\n"
          ]
        }
      ],
      "source": [
        "window_size = 20\n",
        "node_id , features,sliding_window , size_vocab , vocabs , label = sliding(window_size)\n",
        "print(size_vocab)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "W7f0MGwmUsdS"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "from collections import defaultdict\n",
        "def doc_tfidf_embedding(doc, vocab=None):\n",
        "    # Tạo TfidfVectorizer\n",
        "    if vocab:\n",
        "        vectorizer = TfidfVectorizer(vocabulary=vocab)\n",
        "    else:\n",
        "        vectorizer = TfidfVectorizer()\n",
        "\n",
        "    # Tính TF-IDF\n",
        "    X = vectorizer.fit_transform([doc])\n",
        "\n",
        "    # Lấy từ vựng\n",
        "    vocab = vectorizer.get_feature_names_out()\n",
        "\n",
        "    # Tạo dictionary với từ vựng và vector embedding\n",
        "    doc_embeds = {word: X[0, i] for i, word in enumerate(vocab)}\n",
        "\n",
        "    return doc_embeds\n",
        "\n",
        "\n",
        "def compute_word_stats(docs ):\n",
        "\n",
        "    Wi = defaultdict(int)\n",
        "    Wij = defaultdict(int)\n",
        "    # Lặp qua từng document\n",
        "    for doc in docs:\n",
        "        # Tạo set các từ trong document để loại bỏ trùng lặp\n",
        "        unique_words = set(doc)\n",
        "\n",
        "        # Cập nhật số lượng documents chứa từng từ\n",
        "        for word in unique_words:\n",
        "            Wi[word] += 1\n",
        "\n",
        "        # Tính toán số lượng documents chứa cả hai từ\n",
        "        for i in range(len(unique_words)):\n",
        "            for j in range(i + 1, len(unique_words)):\n",
        "                word_i = list(unique_words)[i]\n",
        "                word_j = list(unique_words)[j]\n",
        "                Wij[(word_i, word_j)] += 1\n",
        "\n",
        "    return Wi, Wij\n",
        "def compute_adj(size_vocab , node_id , sliding_window ):\n",
        "    N = len(list(node_id.keys()))\n",
        "    adj = np.eye(N , dtype=float)\n",
        "    docs , label = get_docs()\n",
        "    for doc in tqdm(docs) :\n",
        "        word_tfidf = doc_tfidf_embedding(doc , vocab = vocabs)\n",
        "        for x in word_tfidf:\n",
        "            try:\n",
        "                adj[node_id[x]][node_id[doc]] = word_tfidf[x]\n",
        "                adj[node_id[doc]][node_id[x]] = word_tfidf[x]\n",
        "            except:\n",
        "                # continue\n",
        "                import pdb\n",
        "                pdb.set_trace()\n",
        "    # compute PMI\n",
        "    word_pair = {}\n",
        "    word_window = {}\n",
        "    len_sw = len(sliding_window)\n",
        "    Wi , Wij = compute_word_stats(sliding_window)\n",
        "    for (word_i, word_j), count in Wij.items():\n",
        "        adj[node_id[word_i]][node_id[word_j]] = np.log(count*len_sw/(Wi[word_i]*Wi[word_j]))\n",
        "    return adj\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jzU0pxrrYXhj",
        "outputId": "a493c34b-61dd-4269-b4dd-b1f24c6170c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 500/500 [00:23<00:00, 21.64it/s]\n"
          ]
        }
      ],
      "source": [
        "adj = compute_adj(size_vocab , node_id , sliding_window )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class GCN(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, dropout=0.5):\n",
        "        super(GCN, self).__init__()\n",
        "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
        "        self.conv2 = GCNConv(hidden_dim, output_dim)\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def forward(self, x, adj , size_vocab):\n",
        "        x = F.relu(self.conv1(x, adj))\n",
        "        x = F.dropout(x, self.dropout, training=self.training)\n",
        "        x = self.conv2(x, adj)\n",
        "        out = x[size_vocab :]\n",
        "        out = F.softmax(out , dim = 0 )\n",
        "        return out\n",
        "\n",
        "class GCNConv(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(GCNConv, self).__init__()\n",
        "        self.weight = nn.Parameter(torch.FloatTensor(input_dim, output_dim))\n",
        "        self.bias = nn.Parameter(torch.FloatTensor(output_dim))\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        nn.init.xavier_uniform_(self.weight)\n",
        "        nn.init.zeros_(self.bias)\n",
        "\n",
        "    def forward(self, x, adj):\n",
        "        x = x @ self.weight\n",
        "        x = adj @ x\n",
        "        x = x + self.bias\n",
        "        return x"
      ],
      "metadata": {
        "id": "ul893jkXGO9s"
      },
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features = torch.Tensor(np.stack(features))\n",
        "adj = torch.Tensor(np.array(adj))\n",
        "print(features.shape)\n",
        "print(adj.shape)"
      ],
      "metadata": {
        "id": "08AtYQR7JwfU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 10\n",
        "\n",
        "model = GCN(50 , 128 , 10  , 0)\n",
        "opt = torch.optim.AdamW(model.parameters() , lr = 1e-3)\n",
        "loss_fc = nn.CrossEntropyLoss()\n",
        "# target = torch.nn.functional.one_hot(torch.tensor(label))\n",
        "target =  torch.tensor(label)\n",
        "for epoch in range(num_epochs):\n",
        "        outputs = model(features , adj, size_vocab )\n",
        "        # outputs = torch.argmax(outputs , dim = 0)\n",
        "        loss = loss_fc(outputs, target)\n",
        "        print(loss)\n",
        "        opt.zero_grad()\n",
        "        loss.backward()\n",
        "        opt.step()"
      ],
      "metadata": {
        "id": "T-IkEC52PueG"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}